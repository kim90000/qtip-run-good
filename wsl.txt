الحمد لله
اشتغل

qtip














Welcome to Ubuntu 24.04.2 LTS (GNU/Linux 5.15.167.4-microsoft-standard-WSL2 x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

 System information as of Fri Mar 21 15:38:28 PDT 2025

  System load:  0.24                Processes:             50
  Usage of /:   3.3% of 1006.85GB   Users logged in:       0
  Memory usage: 1%                  IPv4 address for eth0: 172.21.151.42
  Swap usage:   0%

 * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar for easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

This message is shown once a day. To disable it please create the
/home/m/.hushlogin file.
m@DESKTOP-MUB16F8:~$
(myenv) m@DESKTOP-MUB16F8:~$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 26 --streaming
/home/m/myenv/bin/python3: Error while finding module specification for 'eval.interactive_gen' (ModuleNotFoundError: No module named 'eval')
(myenv) m@DESKTOP-MUB16F8:~$ dir
m  myenv  qtip
(myenv) m@DESKTOP-MUB16F8:~$ cd qtip
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 15:46:18.321752 1040 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 15:46:18.321945 1040 utils.py:162] NumExpr defaulting to 16 threads.
I0321 15:46:18.699856 1040 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
  File "/home/m/qtip/model/llama.py", line 314
    if ("skip_list" not in config.quip_params.keys())
                                                     ^
SyntaxError: expected ':'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 15:48:09.349039 1135 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 15:48:09.349256 1135 utils.py:162] NumExpr defaulting to 16 threads.
I0321 15:48:09.569986 1135 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
  File "/home/m/qtip/model/llama.py", line 314
    if ("skip_list" not in config.quip_params.keys()
       ^
SyntaxError: '(' was never closed
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 15:50:57.819579 1231 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 15:50:57.819745 1231 utils.py:162] NumExpr defaulting to 16 threads.
I0321 15:50:58.175308 1231 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
  File "/home/m/qtip/model/llama.py", line 314
    (config.quip_params['skip_list'] is None):
                                             ^
SyntaxError: invalid syntax
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 15:56:26.483790 1359 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 15:56:26.483972 1359 utils.py:162] NumExpr defaulting to 16 threads.
I0321 15:56:26.851289 1359 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
  File "/home/m/qtip/model/llama.py", line 315
    config.quip_params["skip_list"] = []
IndentationError: expected an indented block after 'if' statement on line 314
(myenv) m@DESKTOP-MUB16F8:~/qtip$  python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 15:58:38.132150 1456 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 15:58:38.132324 1456 utils.py:162] NumExpr defaulting to 16 threads.
I0321 15:58:38.527375 1456 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
  File "/home/m/qtip/model/llama.py", line 315
    config.quip_params["skip_list"] = []
IndentationError: expected an indented block after 'if' statement on line 314
(myenv) m@DESKTOP-MUB16F8:~/qtip$  python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 15:59:48.106148 1546 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 15:59:48.106377 1546 utils.py:162] NumExpr defaulting to 16 threads.
I0321 15:59:48.490828 1546 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
  File "/home/m/qtip/model/llama.py", line 314
    if config.quip_params["skip_list"] is None:
                                               ^
IndentationError: unindent does not match any outer indentation level
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 16:03:22.717945 1644 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:03:22.718117 1644 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:03:23.089759 1644 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
  File "/home/m/qtip/model/llama.py", line 314
    if config.quip_params["skip_list"] is None:
                                               ^
IndentationError: unindent does not match any outer indentation level
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 16:05:49.961859 1738 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:05:49.962105 1738 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:05:50.345714 1738 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
  File "/home/m/qtip/model/llama.py", line 314
    if ("skip_list" not in config.quip_params.keys()) or (config.quip_params['skip_list'] is None):
IndentationError: unexpected indent
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 16:09:11.979562 1839 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:09:11.979768 1839 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:09:12.342856 1839 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
  File "/home/m/qtip/model/llama.py", line 454
    if ("skip_list" not in config.quip_params.keys()) or (config.quip_params['skip_list'] is None):
IndentationError: unexpected indent
(myenv) m@DESKTOP-MUB16F8:~/qtip$  python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 16:12:13.212220 1970 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:12:13.212396 1970 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:12:13.581980 1970 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
  File "/home/m/qtip/model/llama.py", line 609, in <module>
    class LlamaFlashAttention2(LlamaAttention):
                               ^^^^^^^^^^^^^^
NameError: name 'LlamaAttention' is not defined
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 16:13:31.874405 2090 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:13:31.874584 2090 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:13:32.248243 2090 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3558, in from_pretrained
    raise EnvironmentError(
OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory relaxml/Llama-2-13b-E8P-2Bit.
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 16:18:21.147518 2280 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:18:21.147699 2280 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:18:21.521880 2280 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
  File "/home/m/qtip/model/llama.py", line 314
    if ("skip_list" not in config.quip_params.keys())
                                                     ^
SyntaxError: expected ':'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 16:20:17.715658 2374 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:20:17.715884 2374 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:20:18.087986 2374 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3558, in from_pretrained
    raise EnvironmentError(
OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory relaxml/Llama-2-13b-E8P-2Bit.
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 16:21:01.390554 2543 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:21:01.390753 2543 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:21:01.588786 2543 config.py:58] PyTorch version 2.4.0 available.
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 1.12k/1.12k [00:00<00:00, 2.81MB/s]
model.safetensors.index.json: 100%|█████████████████████████████████████████████████████████████████████████████████████| 76.4k/76.4k [00:00<00:00, 4.99MB/s]
Downloading shards:   0%|                                                                                                              | 0/2 [00:00<?, ?it/s^model-00001-of-00002.safetensors:  68%|███████████████████████████████████████████████████████                          | 3.09G/4.55G [14:43<06:55, 3.50MB/s]
Downloading shards:   0%|                                                                                                              | 0/2 [14:43<?, ?it/s]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3769, in from_pretrained
    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/utils/hub.py", line 1098, in get_checkpoint_shard_files
    cached_filename = cached_file(
                      ^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1389, in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1915, in _download_to_tmp_and_move
    http_get(
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 549, in http_get
    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):
  File "/home/m/myenv/lib/python3.12/site-packages/requests/models.py", line 820, in generate
    yield from self.raw.stream(chunk_size, decode_content=True)
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 1066, in stream
    data = self.read(amt=amt, decode_content=decode_content)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 955, in read
    data = self._raw_read(amt)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 879, in _raw_read
    data = self._fp_read(amt, read1=read1) if not fp_closed else b""
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 862, in _fp_read
    return self._fp.read(amt) if amt is not None else self._fp.read()
           ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 479, in read
    s = self.fp.read(amt)
        ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/ssl.py", line 1252, in recv_into
    return self.read(nbytes, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/ssl.py", line 1104, in read
    return self._sslobj.read(len, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
^C
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 16:36:14.270740 2798 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:36:14.270930 2798 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:36:14.600079 2798 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
  File "/home/m/qtip/model/llama.py", line 314
    if ("skip_list" not in config.quip_params.keys())
                                                     ^
SyntaxError: expected ':'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 25 --streaming
I0321 16:37:53.786738 2886 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:37:53.786918 2886 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:37:54.160624 2886 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3558, in from_pretrained
    raise EnvironmentError(
OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory relaxml/Llama-2-13b-E8P-2Bit.
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 16:38:23.430068 3055 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:38:23.430298 3055 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:38:23.663930 3055 config.py:58] PyTorch version 2.4.0 available.
model-00001-of-00002.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████| 4.55G/4.55G [06:58<00:00, 3.48MB/s]
model-00002-of-00002.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████| 1.05G/1.05G [05:00<00:00, 3.50MB/s]
Downloading shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [11:58<00:00, 359.14s/it]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3886, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1355, in __init__
    self.model = LlamaModel(config)
                 ^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1136, in __init__
    LlamaDecoderLayer(config, layer_idx)
  File "/home/m/qtip/model/llama.py", line 915, in __init__
    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 455, in __init__
    if config.quip_params['skip_list'] is None:
       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
KeyError: 'skip_list'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 16:52:16.111641 3245 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:52:16.111813 3245 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:52:16.454473 3245 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
  File "/home/m/qtip/model/llama.py", line 314
    if ("skip_list" not in config.quip_params.keys())
                                                     ^
SyntaxError: expected ':'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 16:54:05.299215 3337 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:54:05.299413 3337 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:54:05.673521 3337 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3886, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1355, in __init__
    self.model = LlamaModel(config)
                 ^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1136, in __init__
    LlamaDecoderLayer(config, layer_idx)
  File "/home/m/qtip/model/llama.py", line 915, in __init__
    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 455, in __init__
    if config.quip_params['skip_list'] is None:
       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
KeyError: 'skip_list'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 16:55:42.217181 3525 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:55:42.217364 3525 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:55:42.592836 3525 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3886, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1355, in __init__
    self.model = LlamaModel(config)
                 ^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1136, in __init__
    LlamaDecoderLayer(config, layer_idx)
  File "/home/m/qtip/model/llama.py", line 915, in __init__
    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 455, in __init__
    if config.quip_params['skip_list'] is None:
       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
KeyError: 'skip_list'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 16:58:52.073143 3719 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 16:58:52.073337 3719 utils.py:162] NumExpr defaulting to 16 threads.
I0321 16:58:52.498139 3719 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
  File "/home/m/qtip/model/llama.py", line 317
    if config.quip_params["skip_list"] is None:
                                               ^
IndentationError: unindent does not match any outer indentation level
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 17:02:16.347425 3803 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 17:02:16.347607 3803 utils.py:162] NumExpr defaulting to 16 threads.
I0321 17:02:16.719887 3803 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3886, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1361, in __init__
    self.model = LlamaModel(config)
                 ^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1142, in __init__
    LlamaDecoderLayer(config, layer_idx)
  File "/home/m/qtip/model/llama.py", line 921, in __init__
    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 461, in __init__
    if config.quip_params['skip_list'] is None:
       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
KeyError: 'skip_list'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 17:04:19.768569 3984 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 17:04:19.768738 3984 utils.py:162] NumExpr defaulting to 16 threads.
I0321 17:04:20.132401 3984 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3886, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1366, in __init__
    self.model = LlamaModel(config)
                 ^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1147, in __init__
    LlamaDecoderLayer(config, layer_idx)
  File "/home/m/qtip/model/llama.py", line 926, in __init__
    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 466, in __init__
    if config.quip_params['skip_list'] is None:
       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
KeyError: 'skip_list'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 17:07:18.693602 4170 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 17:07:18.693853 4170 utils.py:162] NumExpr defaulting to 16 threads.
I0321 17:07:19.064925 4170 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 10, in <module>
    from lib.utils.unsafe_import import model_from_hf_path
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 10, in <module>
    from model.llama import LlamaForCausalLM
ImportError: cannot import name 'LlamaForCausalLM' from 'model.llama' (/home/m/qtip/model/llama.py)
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 17:08:27.692471 4283 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 17:08:27.692638 4283 utils.py:162] NumExpr defaulting to 16 threads.
I0321 17:08:28.086536 4283 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3408, in from_pretrained
    config, model_kwargs = cls.config_class.from_pretrained(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'from_pretrained'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 17:09:36.600985 4460 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 17:09:36.601211 4460 utils.py:162] NumExpr defaulting to 16 threads.
I0321 17:09:36.991274 4460 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3408, in from_pretrained
    config, model_kwargs = cls.config_class.from_pretrained(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'from_pretrained'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 17:11:06.726632 4636 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 17:11:06.726842 4636 utils.py:162] NumExpr defaulting to 16 threads.
I0321 17:11:06.929074 4636 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3880, in from_pretrained
    config = cls._autoset_attn_implementation(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 1581, in _autoset_attn_implementation
    config = cls._check_and_enable_sdpa(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 1776, in _check_and_enable_sdpa
    raise ValueError(
ValueError: LlamaForCausalLM does not support an attention implementation through torch.nn.functional.scaled_dot_product_attention yet. Please request the support for this architecture: https://github.com/huggingface/transformers/issues/28005. If you believe this error is a bug, please open an issue in Transformers GitHub repository and load your model with the argument `attn_implementation="eager"` meanwhile. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="eager")`
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 17:18:28.916500 4821 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 17:18:28.916672 4821 utils.py:162] NumExpr defaulting to 16 threads.
I0321 17:18:29.283103 4821 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3886, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1354, in __init__
    self.model = LlamaModel(config)
                 ^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1135, in __init__
    LlamaDecoderLayer(config, layer_idx)
  File "/home/m/qtip/model/llama.py", line 914, in __init__
    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 454, in __init__
    if config.quip_params['skip_list'] is None:
       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
KeyError: 'skip_list'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 17:20:11.710617 5002 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 17:20:11.710837 5002 utils.py:162] NumExpr defaulting to 16 threads.
I0321 17:20:12.085347 5002 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3886, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1355, in __init__
    self.model = LlamaModel(config)
                 ^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1136, in __init__
    LlamaDecoderLayer(config, layer_idx)
  File "/home/m/qtip/model/llama.py", line 915, in __init__
    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 455, in __init__
    if config.quip_params['skip_list'] is None:
       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
KeyError: 'skip_list'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 17:21:47.965690 5184 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 17:21:47.965872 5184 utils.py:162] NumExpr defaulting to 16 threads.
I0321 17:21:48.340692 5184 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3886, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1355, in __init__
    self.model = LlamaModel(config)
                 ^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1136, in __init__
    LlamaDecoderLayer(config, layer_idx)
  File "/home/m/qtip/model/llama.py", line 915, in __init__
    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 455, in __init__
    if config.quip_params['skip_list'] is None:
       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
KeyError: 'skip_list'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 17:28:44.254499 5375 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 17:28:44.254683 5375 utils.py:162] NumExpr defaulting to 16 threads.
I0321 17:28:44.632725 5375 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3886, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1354, in __init__
    self.model = LlamaModel(config)
                 ^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1135, in __init__
    LlamaDecoderLayer(config, layer_idx)
  File "/home/m/qtip/model/llama.py", line 914, in __init__
    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 454, in __init__
    if config.quip_params['skip_list'] is None:
       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
KeyError: 'skip_list'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit --max_new_tokens 128 --streaming
I0321 17:38:38.635444 5547 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0321 17:38:38.635629 5547 utils.py:162] NumExpr defaulting to 16 threads.
I0321 17:38:39.007766 5547 config.py:58] PyTorch version 2.4.0 available.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.52it/s]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit and are newly initialized: ['model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tp_rank']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
generation_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 184/184 [00:00<00:00, 680kB/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:21<00:00, 10.93s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at relaxml/Llama-3.1-8b-Instruct-QTIP-4Bit and are newly initialized: ['model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tp_rank']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 55.4k/55.4k [00:00<00:00, 5.60MB/s]
tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 9.09M/9.09M [00:02<00:00, 3.30MB/s]
special_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 296/296 [00:00<00:00, 1.21MB/s]
W0321 17:39:16.002000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.002000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.003000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.003000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.003000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q3 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.003000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.003000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.030000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.031000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.031000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.031000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.031000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.321000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.322000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.322000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.322000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.322000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.951000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.951000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.952000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.952000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.952000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.952000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z3 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.953000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.972000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.972000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.972000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.972000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:16.973000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:17.184000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:17.184000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:17.184000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:17.185000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:17.185000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:19.931000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:19.932000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x3 is not in var_ranges, defaulting to unknown range.
W0321 17:39:19.932000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:19.932000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:19.932000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:19.932000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:19.933000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:19.956000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:19.956000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:19.957000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:19.957000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:19.957000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:20.615000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:20.615000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:20.615000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:20.616000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:20.616000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.752000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.753000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.753000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q3 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.753000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.753000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.753000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.754000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.782000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.782000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.782000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.782000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.782000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.940000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.940000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.940000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.940000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:28.940000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.164000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.164000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.165000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.165000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.165000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.165000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z3 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.165000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.185000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.185000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.185000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.185000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.185000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.248000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.248000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.248000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.249000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:29.249000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.324000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.640000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.641000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x3 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.641000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.641000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.641000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.641000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.642000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.663000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.664000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.664000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.664000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.664000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.916000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.916000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.916000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.916000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:30.916000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:31.385000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:38.785000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:38.785000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:38.786000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q3 is not in var_ranges, defaulting to unknown range.
W0321 17:39:38.786000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:38.786000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:38.786000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:38.786000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:38.830000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:38.830000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:38.830000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:38.830000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:38.830000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.001000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.001000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.001000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.002000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.002000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.329000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.329000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.329000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.329000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.329000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.329000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z3 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.330000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.363000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.363000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.364000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.364000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.364000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.435000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.435000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.435000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.435000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:39.435000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:40.836000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.309000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.309000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.309000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.310000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.310000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.310000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.310000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.344000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.344000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.345000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.345000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.345000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.813000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.814000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.814000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.814000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0321 17:39:41.814000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:42.365000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0321 17:39:42.371000 139621928251520 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps4 is not in var_ranges, defaulting to unknown range.
W0321 17:39:48.059643 5547 logging.py:328] Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
W0321 17:39:51.795625 5547 warnings.py:110] /usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)

W0321 17:39:55.762919 5547 warnings.py:110] /usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)

Capturing CUDA graphs, may take some time. If you are running a model over multiple GPUs, the first generation will be very slow due to compiling the model.
W0321 17:40:43.321000 139621928251520 torch/_inductor/utils.py:977] [2/0] Not enough SMs to use max_autotune_gemm mode
W0321 17:41:38.776284 5547 warnings.py:110] /usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)

W0321 17:41:39.729280 5547 warnings.py:110] /usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)

What is your prompt? Who is Napoleon Bonaparte?
W0321 17:42:08.246246 5547 warnings.py:110] /usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)

Napoleon Bonaparte (1769-1821) was a French military and political leader who rose to prominence during the French Revolution and its associated wars in Europe. He is considered one of the most influential and successful military strategists in history. Born in Corsica, Napoleon Bonaparte was a brilliant tactician and a skilled leader who seized power in a coup d'état in 1799 and became the Emperor of France in 1804.

**Early Life and Career:**

Napoleon was born on August 15, 1769, in Ajaccio, Corsica, to a minor Corsican noble
Decoding throughput: 128.17 tokens/sec. Includes tokens generated after the EOS token.


What is your prompt?

